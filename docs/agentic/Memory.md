# ðŸ“ Memory & Context Management

This document explores how the system maintains "Infinite Presence" within the finite constraints of LLM context windows.

---

## ðŸ”— The Scratchpad (Pass-by-Reference)
The **Scratchpad** is the central nervous system for data handling. It allows the agent to manipulate massive payloads without "choking" the prompt.

### â“ The Problem
Standard LLMs fail on massive data due to:
*   ðŸ’¸ **Cost**: High token consumption.
*   ðŸ§  **Attention**: "Lost in the middle" phenomenon.
*   ðŸ’¥ **Structural Failure**: Giant strings breaking JSON tool definitions.

### ðŸ”„ The "Reference-Life" Cycle
1.  **ðŸ“¥ Write**: The Input Gatekeeper saves giant files â®• ðŸ“ **Scratchpad**.
2.  **ðŸ”— Link**: A lightweight `ref:key` is passed to the LLM.
3.  **ðŸ”Ž Discovery**: The Agent uses `scratchpad_list` to see what is stored.
4.  **âœ‚ï¸ Granular Access**: Agent reads exactly what it needs via `scratchpad_read_chunk`.
5.  **âš™ï¸ Execution**: References are resolved *only* at the point of tool execution.

---

## ðŸ–‡ï¸ Context Optimization & Compression
When the conversation gets "heavy," the **Context Compressor** utility (`ContextCompressor` in `cli/cowork/agent.py`) activates automatically.

### Compression Safety Contract (Current)
1. **No compression without reference**:
   - Before conversation compression, full source history is archived to scratchpad with a named `ref:key`.
   - The injected summary includes that source pointer.
2. **No summary-of-summary loops**:
   - Prior `[CONVERSATION SUMMARY]` system messages are excluded from future compression input.
3. **No scratchpad-of-scratchpad loops**:
   - Outputs already marked with `[Full result saved as ref:...]` are not compressed again.

### 1ï¸âƒ£ Atomic Compression (Surge Protection)
Identifies **"Heavy Nodes"**â€”single messages (e.g., a huge SQL output) occupying >75% of the window.
*   ðŸŽ¯ **Action**: Fragments the message for targeted reduction.
*   ðŸŒ¡ï¸ **Precision**: Runs at **Temp 0.1** to protect facts and numbers.

### 2ï¸âƒ£ History Rolling Window (Deep Cleanup)
Synthesizes older parts of the conversation into a coherent narrative block.
*   ðŸ›¡ï¸ **Identity Protection**: The System Identity and the **Last 2 Human Messages** are never compressed.

---

## ðŸ“‰ Compression Logic (Map-Reduce)
We use a recursive pipeline to ensure that reduction doesn't equal loss of intelligence.

```mermaid
graph TD
    A[ðŸ Step Start] --> B{ðŸš¦ Tokens > Buffer?}
    B -- No --> C[ðŸƒ Execute Turn]
    B -- Yes --> D[ðŸ” Identify Heavy History]
    
    subgraph "Compression Phase (Map-Reduce)"
        D --> E[âœ‚ï¸ Smart Chunking]
        E --> F["Fragment 1..N (Boundaries)"]
        F --> G[ðŸ§  LLM Map: Summarize]
        G --> H[ðŸ“¦ Combined Result]
        H --> I["ðŸ“‰ LLM Reduce: De-duplicate"]
    end
    
    I --> J[ðŸ“ Generate Summary Block]
    J --> K["ðŸ’‰ Inject [CONVERSATION SUMMARY]"]
    K --> L[ðŸ§¹ Prune History]
    L --> M[ðŸ”„ Resume REACT Loop]
```

### âœ‚ï¸ Smart Chunking
The `_smart_chunk` utility identifies **semantic boundaries** (newlines, sentence ends) rather than cutting mid-word, ensuring context flows correctly through the Map phase.

---

## ðŸ¥ª The Sandwich Preview
For instant feedback without LLM overhead, the system uses a **Sandwich Reduction**:
- ðŸŸ¢ **Head**: The introduction/header.
- ðŸŸ¡ **Middle**: A core representative snippet.
- ðŸ”´ **Tail**: The conclusion/bottom data.

> **Note**: This is the default format for the **Input Gatekeeper** preview.
