# ğŸŒ Agentic System Overview

## ğŸš€ Introduction
The **Makix Enterprise Agentic System** is a high-performance, resilient AI orchestration layer. It is engineered to overcome LLM context window limits and statelessness through two core innovations:

1.  ğŸ­ **"Manager-Worker" Persona**: Enforces coordination over verbosity.
2.  ğŸ”— **"Pass-by-Reference" Memory**: Handles massive data via pointer-based logic.

---

## ğŸ›ï¸ Global Architecture
The system is partitioned into three functional "Cerebral Zones":

### ğŸ›¡ï¸ Zone 1: Ingestion & Protection
*Gatekeeping the context window.*

### ğŸ§  Zone 2: Preparation (The Brain)
*Intent analysis and tool selection.*

### ğŸ› ï¸ Zone 3: Execution (The Worker)
*Recursive reasoning and tool execution.*

```mermaid
graph TD
    User((ğŸ‘¤ User)) -->|Input| Gatekeeper[ğŸ›¡ï¸ Input Gatekeeper]

    subgraph "Phase 1: Ingestion & Protection"
        Gatekeeper -- "> Limit" --> Offload[ğŸ“ Offload to Scratchpad]
        Offload -->|Ref Key| JobMgr
        Gatekeeper -- "Valid" --> JobMgr{âš™ï¸ Agent Job Manager}
        
        JobMgr -->|Persist State| Storage[(ğŸ’¾ localStorage)]
        JobMgr -->|Queue Check| Queue{ğŸš¦ Queue < 10?}
    end

    subgraph "Phase 2: Preparation (The Brain)"
        Queue -- Yes --> Router[ğŸ§­ Meta-Router]
        Router -->|Temp 0.0| Classifier[ğŸ” Intent Classifier]
        Classifier -->|Select| Tools[ğŸ› ï¸ Tool Schema Loading]
        Classifier -->|Inject| Actions[âš¡ Action Instructions]
    end

    subgraph "Phase 3: Execution Loop (The Worker)"
        Tools --> Agent[ğŸ¤– General Purpose Agent]
        Actions --> Agent
        
        Agent -->|1. Check Context| Compressor[ğŸ–‡ï¸ Context Compressor]
        Compressor -- "Atomic Map-Reduce" --> LLM_S[ğŸ“‰ LLM Temp 0.1]
        LLM_S -->|Summary| Agent
        
        Agent -->|2. Generate| LLM_G[ğŸ§  LLM Temp 0.4]
        LLM_G -->|Tool Calls| Gateway{ğŸš§ Execution Gateway}
        
        Gateway -->|Resolve Refs| SP[(ğŸ“ Scratchpad)]
        Gateway -->|Execute| Executor[âš™ï¸ Tool Executor]
        
        Executor -- "Result > Limit" --> Sandwich[ğŸ¥ª Sandwich Preview]
        Sandwich --> Agent
        Executor -- "Result < Limit" --> Agent
    end

    Agent -->|Final Result| JobMgr
    JobMgr -->|Clear Persistence| Storage
    JobMgr -->|Dispatch| Notification[ğŸ”” Notification System]
    Notification -->|Render| UI[ğŸ’» Chat UI]
```

---

## ğŸ“š Documentation Index
| Module | Focus | Link |
| :--- | :--- | :--- |
| ğŸ”„ **Workflow** | Phase-by-phase request lifecycle | [View Workflow](./Workflow.md) |
| ğŸ“ **Memory** | Scratchpad & Compression logic | [View Memory](./Memory.md) |
| ğŸ§  **Intelligence** | Routing & Temperature tiers | [View Intelligence](./Intelligence.md) |
| ğŸ›¡ï¸ **Operations** | Queue, Persistence & Safety | [View Operations](./Operations.md) |

---

## ğŸ’ Core Philosophical Pillars
- ğŸ’° **Context is Currency**: Don't spend tokens on raw data unless required for reasoning.
- ğŸ¯ **Precision over Creativity**: Logic tiers (routing, compression) run at near-zero temperature.
- âš¡ **Parallel-First Orchestration**: Intent classification, memory retrieval, and tool discovery run concurrently to minimize "time-to-first-token."
- ğŸ’¾ **Persistence & Caching**: Every job is synced to survive crashes, and user context is cached to eliminate redundant Auth round-trips.
- ğŸ”Š **Fail Loudly & Recursively**: Errors are fed back as observations for AI self-healing.
